\begin{sidewaystable}
  \centering
  \setlength{\tabcolsep}{6pt}
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}{lcccccccc}
    \toprule
    \multicolumn{9}{c}{\cellcolor{ggufColor} \textbf{Accuracy metrics}} \\
    \midrule
    \makecell{\textbf{Model}\\\textbf{name}} &
    \makecell{\textbf{Model}\\\textbf{Params}} &
    \makecell{\textbf{RAG}\\\textbf{type}} &
    \makecell{\textbf{Device}\\\textbf{used}} &
    \makecell{\textbf{Quant}\\\textbf{scheme}} &
    \makecell{\textbf{Binary}\\\textbf{output}} &
    \makecell{\textbf{Structure}\\\textbf{followed (\%)}} &
    \makecell{\textbf{Accuracy}\\\textbf{(avg. | parsed)}} &
    \makecell{\textbf{Output tokens}\\\textbf{(avg. | parsed)}} \\
    \midrule
    Llama3.1-NI & 8.03 B & \xmark & GPU & FP16 & \xmark & 75.25\% & \hphantom{}33.88 | 44.58\hphantom{}\% & \hphantom{}508 | 508\hphantom{} \\
    Llama3.1-NI & 8.03 B & OpenAI & GPU & FP16 & \xmark & 88.64\% & \hphantom{}40.93 | 46.32\hphantom{}\% & \hphantom{}510 | 513\hphantom{} \\
    Llama3.1-NI & 8.03 B & BGE & GPU & FP16 & \xmark & 83.76\% & \hphantom{}37.88 | 45.62\hphantom{}\% & \hphantom{}501 | 512\hphantom{} \\
    Llama3.1-NI & 8.03 B & \xmark & GPU & FP16 & \cmark & 2.66\% & \hphantom{0}1.33 | 57.42\hphantom{}\% & \hphantom{}252 | 7\hphantom{00} \\
    Llama3.1-NI & 8.03 B & OpenAI & GPU & FP16 & \cmark & 9.26\% & \hphantom{0}2.09 | 44.24\hphantom{}\% & \hphantom{}326 | 7\hphantom{00} \\
    Llama3.1-NI & 8.03 B & BGE & GPU & FP16 & \cmark & 5.33\% & \hphantom{0}1.90 | 43.03\hphantom{}\% & \hphantom{}344 | 7\hphantom{00} \\
    \midrule
    Llama3.1 & 8.03 B & \xmark & GPU & FP16 & \xmark & 100.00\% & \hphantom{}71.38 | 71.38\hphantom{}\% & \hphantom{}129 | 129\hphantom{} \\
    Llama3.1 & 8.03 B & OpenAI & GPU & FP16 & \xmark & 99.56\% & \hphantom{}68.34 | 68.74\hphantom{}\% & \hphantom{}137 | 135\hphantom{} \\
    Llama3.1 & 8.03 B & BGE & GPU & FP16 & \xmark & 99.81\% & \hphantom{}67.70 | 67.88\hphantom{}\% & \hphantom{}139 | 139\hphantom{} \\
    Llama3.1 & 8.03 B & \xmark & GPU & FP16 & \cmark & 50.82\% & \hphantom{}37.94 | 75.39\hphantom{}\% & \hphantom{0}10 | 7\hphantom{00} \\
    Llama3.1 & 8.03 B & OpenAI & GPU & FP16 & \cmark & 60.15\% & \hphantom{}40.80 | 70.72\hphantom{}\% & \hphantom{0}13 | 7\hphantom{00} \\
    Llama3.1 & 8.03 B & BGE & GPU & FP16 & \cmark & 61.17\% & \hphantom{}44.73 | 76.05\hphantom{}\% & \hphantom{0}11 | 7\hphantom{00} \\
    \midrule
    Llama3.1 & 8.03 B & \xmark & Axelera & INT8 & \xmark & 87.12\% & \hphantom{}55.20 | 63.64\hphantom{}\% & \hphantom{0}85 | 87\hphantom{0} \\
    Llama3.1 & 8.03 B & OpenAI & Axelera & INT8 & \xmark & 79.63\% & \hphantom{}52.54 | 65.69\hphantom{}\% & \hphantom{}109 | 106\hphantom{} \\
    Llama3.1 & 8.03 B & BGE & Axelera & INT8 & \xmark & 85.22\% & \hphantom{}59.20 | 69.90\hphantom{}\% & \hphantom{}102 | 98\hphantom{0} \\
    Llama3.1 & 8.03 B & \xmark & Axelera & INT8 & \cmark & 65.93\% & \hphantom{}40.61 | 59.96\hphantom{}\% & \hphantom{0}25 | 7\hphantom{00} \\
    Llama3.1 & 8.03 B & OpenAI & Axelera & INT8 & \cmark & 65.29\% & \hphantom{}43.59 | 64.39\hphantom{}\% & \hphantom{0}25 | 7\hphantom{00} \\
    Llama3.1 & 8.03 B & BGE & Axelera & INT8 & \cmark & 68.97\% & \hphantom{}43.78 | 61.88\hphantom{}\% & \hphantom{0}24 | 7\hphantom{00} \\
    \bottomrule
  \end{tabular}
  \caption{Comparison of Llama3.1-8B models.}
  \label{tab:llama-8b}
\end{sidewaystable}
